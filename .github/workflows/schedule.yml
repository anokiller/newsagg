name: Run Tech News Aggregator

on:
  schedule:
    - cron: '0 */6 * * *'  # Runs every 6 hours
  workflow_dispatch:  # Allows manual triggering in the Actions tab

jobs:
  run-technews:
    runs-on: ubuntu-latest

    steps:
      # 1) Checkout your repository
      - name: Check out code
        uses: actions/checkout@v3

      # 2) Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'  # Adjust as needed

      # 3) Download existing artifact
      - name: Download processed URLs artifact
        uses: actions/download-artifact@v4
        with:
          name: processed-urls.json
          path: .
        # Allow the workflow to continue even if no artifact is found (i.e., first run)
        continue-on-error: true

      # 4) Cache pip dependencies (optional but recommended)
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      # 5) Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 6) Run technewsagg.py
      - name: Run Tech News Aggregator
        env:
          BOT_API_TOKEN: ${{ secrets.BOT_API_TOKEN }}
          CHAT_ID: ${{ secrets.CHAT_ID }}
        run: |
          # If your script expects a file named processed_urls.json in the repo root:
          # python technewsagg.py
          
          # Or if you need to specify the file path:
          # python technewsagg.py --processed-urls ./processed_urls.json
          
          python technewsagg.py

      # 7) Upload updated processed_urls.json
      - name: Upload processed URLs artifact
        uses: actions/upload-artifact@v4
        with:
          name: processed-urls.json
          path: .
